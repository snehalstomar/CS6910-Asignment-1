# -*- coding: utf-8 -*-
"""Untitled4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/129DOpttzmiaHiks-KhumWT6qDllDd3EV
"""

def momemtum_grad_descent(n_hidden_layers, n_inputs, n_outputs,x,y,epoch,eta,gamma,prev_w ,prev_b):
    	'''
		x: input vector --- all inputs
		y: training class -vectors
		'''
    #prev_w = {}
		#prev_b = {}
		nn = myDLkit.feed_fwd_nn(n_hidden_layers, n_inputs, n_outputs)
		nn.backProp(x, y)
		l = len((nn.layers).b)
		for i in nn.grad_wrt_W:
    		nn.grad_wrt_W[i].T = gamma * prev_w['w'+ str(i)] + eta * nn.grad_wrt_W[i].T
				nn.layers[i].W = nn.layers[i].W - nn.grad_wrt_W[i].T 
				prev_w['w' + str(i)] = nn.grad_wrt_W[i].T

		for i in nn.grad_wrt_b:
    		nn.grad_wrt_b[i].T = gamma * prev_b['pb' + str(i)] + eta * nn.grad_wrt_b[i].T
				nn.layers[i].b = nn.layers[i].b - nn.grad_wrt_b[i].T 
				prev_b['pb' + str(i)] = nn.grad_wrt_b[i].T

		return prev_w,prev,nn.layers